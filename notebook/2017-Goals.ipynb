{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script> \n",
    "code_show=true;\n",
    "function code_toggle() { \n",
    "  if (code_show){\n",
    "    $('div.input').hide();\n",
    "    $('div.prompt').hide();\n",
    "  } \n",
    "  else { \n",
    "    $('div.prompt').show();\n",
    "    $('div.input').show();\n",
    "    } code_show = !code_show ;\n",
    "    \n",
    "}\n",
    "\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<style>\n",
    "input {\n",
    "    width : 100%;\n",
    "    background-color: white;\n",
    "    border: none;\n",
    "    color: white;\n",
    "    padding: 1px 1px;\n",
    "    margin: 1px 1px;\n",
    "    cursor: pointer;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<form action=\"javascript:code_toggle()\">\n",
    "  <input type=\"submit\" value=\"\">\n",
    "</form>''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 2017 Plan\n",
    "- Reinforcement Learning \n",
    " - \n",
    "- Paper Reading\n",
    "- Personal AI project ( investment )\n",
    "- Data Scientist Blog Refine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measurement\n",
    "\n",
    "- $ \\text{EPS (Earnings Per Share) }  = \\frac{ \\text{Net Income - Divdends on Preferred Stock }}{\\text{Average Outstanding Shares}} $ \n",
    "\n",
    "- $ \\text{P/E ratio} = \\frac{\\text{Price Per Share ( Stock Price )}}{\\text{EPS}} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operation Detail \n",
    "\n",
    "- 何時持股，才有股息股利？ : 前一天\n",
    "- 股票手續費\n",
    " - Buy  :: 0.1425% \n",
    " - Sell :: 0.1425 + 0.3)%\n",
    " - **All :: 0.6% **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from yahoo_finance import Share\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "def getStock(id):\n",
    "    stock = Share(str(id)+'.TW')\n",
    "    today = datetime.date.today()\n",
    "    data = stock.get_historical('2016-12-01', str(today))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = getStock(2002)\n",
    "df = pd.DataFrame(data)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.ix[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['High'] = df['High'].astype(float)\n",
    "df['Low']  = df['Low'].astype(float)\n",
    "df['Close']= df['Close'].astype(float)\n",
    "df['Open'] = df['Open'].astype(float)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "html = \"\"\"\n",
    "<div id=\"main\" style=\"width:600px;height:400px;\"></div>\n",
    "\n",
    "<script>   \n",
    "\n",
    "//\n",
    "require.config({\n",
    "         paths:{\n",
    "            echarts: ['//cdn.bootcss.com/echarts/3.4.0/echarts.min'],\n",
    "         }\n",
    "    });\n",
    "\n",
    "//\n",
    "require(['echarts'],function(ec){\n",
    "  //\n",
    "  var myChart = ec.init(document.getElementById('main'));\n",
    "\n",
    "  //\n",
    "  var option = {\n",
    "            title: {\n",
    "                text: 'Kent'\n",
    "            },\n",
    "            tooltip: {},\n",
    "            legend: {\n",
    "                data:['销量']\n",
    "            },\n",
    "            xAxis: {\n",
    "                data: [\"衬衫\",\"羊毛衫\",\"雪纺衫\",\"裤子\",\"高跟鞋\",\"袜子\"]\n",
    "            },\n",
    "            yAxis: {},\n",
    "            series: [{\n",
    "                name: '销量',\n",
    "                type: 'bar',\n",
    "                data: [5, 20, 36, 10, 10, 20]\n",
    "            }]\n",
    "        };\n",
    "\n",
    "  //  \n",
    "  myChart.setOption(option);\n",
    "         });\n",
    "    </script>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from ipywidgets import widgets, IntSlider\n",
    "\n",
    "def f(i):\n",
    "    load = 1000000\n",
    "    i = str(i)\n",
    "    pay_per_mon = {'7':13209, '5':17964, '3':29077}\n",
    "    try:\n",
    "        all_paid= pay_per_mon[i] * int(i) * 12\n",
    "        extra_paid = all_paid - load \n",
    "        print extra_paid\n",
    "        avg_earn_rate_peryer = round((72/float(extra_paid)*100),5)\n",
    "        print ('{} %'.format(avg_earn_rate_peryer,))\n",
    "    except:\n",
    "        print ('Do not have this value')\n",
    "    \n",
    "interact(f,i=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recurrent Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# set random seed for comparing the two result calculations\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "# this is data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "\n",
    "n_inputs = 28   # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28    # time steps\n",
    "n_hidden_units = 128   # neurons in hidden layer\n",
    "n_classes = 10      # MNIST classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def RNN(X, weights, biases):\n",
    "    # hidden layer for input to cell\n",
    "    ########################################\n",
    "\n",
    "    # transpose the inputs shape from\n",
    "    # X ==> (128 batch * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    # X_in = (128 batch * 28 steps, 128 hidden)\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batch, 28 steps, 128 hidden)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "\n",
    "    # cell\n",
    "    ##########################################\n",
    "\n",
    "    # basic LSTM Cell.\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    # lstm cell is divided into two parts (c_state, h_state)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    # You have 2 options for following step.\n",
    "    # 1: tf.nn.rnn(cell, inputs);\n",
    "    # 2: tf.nn.dynamic_rnn(cell, inputs).\n",
    "    # If use option 1, you have to modified the shape of X_in, go and check out this:\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    # In here, we go for option 2.\n",
    "    # dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as X_in.\n",
    "    # Make sure the time_major is changed accordingly.\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    #############################################\n",
    "    # results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "\n",
    "    # # or\n",
    "    # unpack to list [(batch, outputs)..] * steps\n",
    "    outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # tf.initialize_all_variables() no long valid from\n",
    "    # 2017-03-02 if using tensorflow >= 0.12\n",
    "    if int((tf.__version__).split('.')[1]) < 12:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape([batch_size, n_steps, n_inputs])\n",
    "        sess.run([train_op], feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        })\n",
    "        if step % 20 == 0:\n",
    "            print(sess.run(accuracy, feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys,\n",
    "        }))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  \"\"\"generate matrix by shape\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"generate matrix by shape\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = weight_variable([3,3])\n",
    "b = bias_variable([3,1])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print a.eval()\n",
    "    print b.eval()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Example\n",
    "\n",
    "### Using tf.SequenceExample to prepare Data in Protocol Buff\n",
    "- RNNs are used for sequential data that has inputs and/or outputs at multiple time steps.\n",
    "\n",
    "### Batching and Padding\n",
    "### Dynamic RNN\n",
    "### Bidirectional Dynamic RNN\n",
    "### RNN Cells and Cell Wrappers\n",
    "### Masking the Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input data with batch = 3 \n",
    "# input data with step = 3 and 2 \n",
    "# input data with dim = ?\n",
    "sequences = [[1, 2, 3], [4, 5, 1], [1, 2]]\n",
    "\n",
    "label_sequences = [[0, 1, 0], [1, 0, 0], [1, 1]]\n",
    "\n",
    "def make_example(sequence, labels):\n",
    "    # The object we return\n",
    "    ex = tf.train.SequenceExample()\n",
    "    # A non-sequential feature of our example\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature[\"length\"].int64_list.value.append(sequence_length)\n",
    "    # Feature lists for the two sequential features of our example\n",
    "    fl_tokens = ex.feature_lists.feature_list[\"tokens\"]\n",
    "    fl_labels = ex.feature_lists.feature_list[\"labels\"]\n",
    "    for token, label in zip(sequence, labels):\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex\n",
    "\n",
    "# Write all examples into a TFRecords file\n",
    "with tempfile.NamedTemporaryFile() as fp:\n",
    "    writer = tf.python_io.TFRecordWriter(fp.name)\n",
    "    for sequence, label_sequence in zip(sequences, label_sequences):\n",
    "        ex = make_example(sequence, label_sequence)\n",
    "        writer.write(ex.SerializeToString())\n",
    "    writer.close()\n",
    "    print(\"Wrote to {}\".format(fp.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# A single serialized example\n",
    "# (You can read this from a file using TFRecordReader)\n",
    "ex = make_example([1, 2, 3], [0, 1, 0]).SerializeToString()\n",
    "\n",
    "# Define how to parse the example\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "# Parse the example (returns a dictionary of tensors)\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(\n",
    "    serialized=ex,\n",
    "    context_features=context_features,\n",
    "    sequence_features=sequence_features\n",
    ")\n",
    "\n",
    "context = tf.contrib.learn.run_n(context_parsed, n=1, feed_dict=None)\n",
    "print(context[0])\n",
    "sequence = tf.contrib.learn.run_n(sequence_parsed, n=1, feed_dict=None)\n",
    "print(sequence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "莫凡RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "BATCH_START = 0\n",
    "TIME_STEPS = 20\n",
    "BATCH_SIZE = 50\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "CELL_SIZE = 10\n",
    "LR = 0.006\n",
    "\n",
    "\n",
    "def get_batch():\n",
    "    global BATCH_START, TIME_STEPS\n",
    "    # xs shape (50batch, 20steps)\n",
    "    xs = np.arange(BATCH_START, BATCH_START+TIME_STEPS*BATCH_SIZE).reshape((BATCH_SIZE, TIME_STEPS)) / (10*np.pi)\n",
    "    seq = np.sin(xs)\n",
    "    res = np.cos(xs)\n",
    "    BATCH_START += TIME_STEPS\n",
    "    plt.plot(xs[0, :], res[0, :], 'r', xs[0, :], seq[0, :], 'b--')\n",
    "    plt.show()\n",
    "    # returned seq, res and xs: shape (batch, step, input)\n",
    "    return [seq[:, :, np.newaxis], res[:, :, np.newaxis], xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### [Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "dataset = pandas.read_csv('/Users/kentchiu/Downloads/international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "plt.plot(dataset)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('/Users/kentchiu/Downloads/international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(train)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1 # Time-series Window Size\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "plt.plot(trainX)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(trainY)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n",
    "\n",
    "Currently, our data is in the form: [samples, features] and we are framing the problem as one time step for each sample. We can transform the prepared train and test input data into the expected structure using numpy.reshape() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (trainX.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=look_back))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform(trainY) \n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "def weight_variable(shape):\n",
    "  \"\"\"generate matrix by shape\"\"\"\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  \"\"\"generate matrix by shape\"\"\"\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "# hyperparameters\n",
    "lr = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "\n",
    "n_inputs = 28   # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28    # time steps\n",
    "n_hidden_units = 128   # neurons in hidden layer\n",
    "n_classes = 10      # MNIST classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # (28, 128)\n",
    "    'in': tf.Variable(tf.random_normal([n_inputs, n_hidden_units])),\n",
    "    # (128, 10)\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    # (128, )\n",
    "    'in': tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ])),\n",
    "    # (10, )\n",
    "    'out': tf.Variable(tf.constant(0.1, shape=[n_classes, ]))\n",
    "}\n",
    "\n",
    "def RNN(X, weights, biases):\n",
    "    # hidden layer for input to cell\n",
    "    ########################################\n",
    "\n",
    "    # transpose the inputs shape from\n",
    "    # X ==> (128 batch * 28 steps, 28 inputs)\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    # X_in = (128 batch * 28 steps, 128 hidden)\n",
    "    X_in = tf.matmul(X, weights['in']) + biases['in']\n",
    "    # X_in ==> (128 batch, 28 steps, 128 hidden)\n",
    "    X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "    #print (X_in.get_shape())\n",
    "\n",
    "    # cell\n",
    "    ##########################################\n",
    "\n",
    "    # basic LSTM Cell.\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(n_hidden_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    # lstm cell is divided into two parts (c_state, h_state)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    # You have 2 options for following step.\n",
    "    # 1: tf.nn.rnn(cell, inputs);\n",
    "    # 2: tf.nn.dynamic_rnn(cell, inputs).\n",
    "    # If use option 1, you have to modified the shape of X_in, go and check out this:\n",
    "    # https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py\n",
    "    # In here, we go for option 2.\n",
    "    # dynamic_rnn receive Tensor (batch, steps, inputs) or (steps, batch, inputs) as X_in.\n",
    "    # Make sure the time_major is changed accordingly.\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    #############################################\n",
    "    # results = tf.matmul(final_state[1], weights['out']) + biases['out']\n",
    "\n",
    "    # # or\n",
    "    # unpack to list [(batch, outputs)..] * steps\n",
    "    outputs = tf.unpack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {
    "c84da1babb1b40819271d26e63ba9687": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
