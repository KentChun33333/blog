{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from typing import Optional\n",
    "from .decoder import DeepLabV3Decoder, DeepLabV3PlusDecoder\n",
    "from ..base import SegmentationModel, SegmentationHead, ClassificationHead\n",
    "from ..encoders import get_encoder\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "__all__ = [\"DeepLabV3Decoder\"]\n",
    "\n",
    "import torch.nn as nn\n",
    "from .modules import Flatten, Activation\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, activation=None, upsampling=1):\n",
    "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
    "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(conv2d, upsampling, activation)\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "\n",
    "    def __init__(self, in_channels, classes, pooling=\"avg\", dropout=0.2, activation=None):\n",
    "        if pooling not in (\"max\", \"avg\"):\n",
    "            raise ValueError(\"Pooling should be one of ('max', 'avg'), got {}.\".format(pooling))\n",
    "        pool = nn.AdaptiveAvgPool2d(1) if pooling == 'avg' else nn.AdaptiveMaxPool2d(1)\n",
    "        flatten = Flatten()\n",
    "        dropout = nn.Dropout(p=dropout, inplace=True) if dropout else nn.Identity()\n",
    "        linear = nn.Linear(in_channels, classes, bias=True)\n",
    "        activation = Activation(activation)\n",
    "        super().__init__(pool, flatten, dropout, linear, activation)\n",
    "\n",
    "class DeepLabV3Decoder(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels=256, atrous_rates=(12, 24, 36)):\n",
    "        super().__init__(\n",
    "            ASPP(in_channels, out_channels, atrous_rates),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, *features):\n",
    "        return super().forward(features[-1])\n",
    "\n",
    "\n",
    "class DeepLabV3PlusDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        out_channels=256,\n",
    "        atrous_rates=(12, 24, 36),\n",
    "        output_stride=16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if output_stride not in {8, 16}:\n",
    "            raise ValueError(\"Output stride should be 8 or 16, got {}.\".format(output_stride))\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.output_stride = output_stride\n",
    "\n",
    "        self.aspp = nn.Sequential(\n",
    "            ASPP(encoder_channels[-1], out_channels, atrous_rates, separable=True),\n",
    "            SeparableConv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        scale_factor = 2 if output_stride == 8 else 4\n",
    "        self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n",
    "\n",
    "        highres_in_channels = encoder_channels[-4]\n",
    "        highres_out_channels = 48   # proposed by authors of paper\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(highres_in_channels, highres_out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(highres_out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            SeparableConv2d(\n",
    "                highres_out_channels + out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, *features):\n",
    "        aspp_features = self.aspp(features[-1])\n",
    "        aspp_features = self.up(aspp_features)\n",
    "        high_res_features = self.block1(features[-4])\n",
    "        concat_features = torch.cat([aspp_features, high_res_features], dim=1)\n",
    "        fused_features = self.block2(concat_features)\n",
    "        return fused_features\n",
    "\n",
    "\n",
    "class ASPPConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=dilation,\n",
    "                dilation=dilation,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "class ASPPSeparableConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, dilation):\n",
    "        super().__init__(\n",
    "            SeparableConv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=dilation,\n",
    "                dilation=dilation,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        for mod in self:\n",
    "            x = mod(x)\n",
    "        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, atrous_rates, separable=False):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        rate1, rate2, rate3 = tuple(atrous_rates)\n",
    "        ASPPConvModule = ASPPConv if not separable else ASPPSeparableConv\n",
    "\n",
    "        modules.append(ASPPConvModule(in_channels, out_channels, rate1))\n",
    "        modules.append(ASPPConvModule(in_channels, out_channels, rate2))\n",
    "        modules.append(ASPPConvModule(in_channels, out_channels, rate3))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels))\n",
    "\n",
    "        self.convs = nn.ModuleList(modules)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(5 * out_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = []\n",
    "        for conv in self.convs:\n",
    "            res.append(conv(x))\n",
    "        res = torch.cat(res, dim=1)\n",
    "        return self.project(res)\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Sequential):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            dilation=1,\n",
    "            bias=True,\n",
    "    ):\n",
    "        dephtwise_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "        pointwise_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=bias,\n",
    "        )\n",
    "        super().__init__(dephtwise_conv, pointwise_conv)\n",
    "        \n",
    "class DeepLabV3(SegmentationModel):\n",
    "    \"\"\"DeepLabV3_ implemetation from \"Rethinking Atrous Convolution for Semantic Image Segmentation\"\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "                extractor to build segmentation model.\n",
    "        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n",
    "            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n",
    "            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n",
    "            spatial resolution (H/(2^depth), W/(2^depth)]\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: a number of convolution filters in ASPP module (default 256).\n",
    "        in_channels: number of input channels for model, default is 3.\n",
    "        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        activation (str, callable): activation function used in ``.predict(x)`` method for inference.\n",
    "            One of [``sigmoid``, ``softmax2d``, callable, None]\n",
    "        upsampling: optional, final upsampling factor\n",
    "            (default is 8 to preserve input -> output spatial shape identity)\n",
    "        aux_params: if specified model will have additional classification auxiliary output\n",
    "            build on top of encoder, supported params:\n",
    "                - classes (int): number of classes\n",
    "                - pooling (str): one of 'max', 'avg'. Default is 'avg'.\n",
    "                - dropout (float): dropout factor in [0, 1)\n",
    "                - activation (str): activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **DeepLabV3**\n",
    "    .. _DeeplabV3:\n",
    "        https://arxiv.org/abs/1706.05587\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_name: str = \"resnet34\",\n",
    "            encoder_depth: int = 5,\n",
    "            encoder_weights: Optional[str] = \"imagenet\",\n",
    "            decoder_channels: int = 256,\n",
    "            in_channels: int = 3,\n",
    "            classes: int = 1,\n",
    "            activation: Optional[str] = None,\n",
    "            upsampling: int = 8,\n",
    "            aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "        self.encoder.make_dilated(\n",
    "            stage_list=[4, 5],\n",
    "            dilation_list=[2, 4]\n",
    "        )\n",
    "\n",
    "        self.decoder = DeepLabV3Decoder(\n",
    "            in_channels=self.encoder.out_channels[-1],\n",
    "            out_channels=decoder_channels,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "\n",
    "class DeepLabV3Plus(SegmentationModel):\n",
    "    \"\"\"DeepLabV3Plus_ implemetation from \"Encoder-Decoder with Atrous Separable\n",
    "Convolution for Semantic Image Segmentation\"\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "                extractor to build segmentation model.\n",
    "        encoder_depth: number of stages used in decoder, larger depth - more features are generated.\n",
    "            e.g. for depth=3 encoder will generate list of features with following spatial shapes\n",
    "            [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have\n",
    "            spatial resolution (H/(2^depth), W/(2^depth)]\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        encoder_output_stride: downsampling factor for deepest encoder features (see original paper for explanation)\n",
    "        decoder_atrous_rates: dilation rates for ASPP module (should be a tuple of 3 integer values)\n",
    "        decoder_channels: a number of convolution filters in ASPP module (default 256).\n",
    "        in_channels: number of input channels for model, default is 3.\n",
    "        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        activation (str, callable): activation function used in ``.predict(x)`` method for inference.\n",
    "            One of [``sigmoid``, ``softmax2d``, callable, None]\n",
    "        upsampling: optional, final upsampling factor\n",
    "            (default is 8 to preserve input -> output spatial shape identity)\n",
    "        aux_params: if specified model will have additional classification auxiliary output\n",
    "            build on top of encoder, supported params:\n",
    "                - classes (int): number of classes\n",
    "                - pooling (str): one of 'max', 'avg'. Default is 'avg'.\n",
    "                - dropout (float): dropout factor in [0, 1)\n",
    "                - activation (str): activation function to apply \"sigmoid\"/\"softmax\" (could be None to return logits)\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **DeepLabV3Plus**\n",
    "    .. _DeeplabV3Plus:\n",
    "        https://arxiv.org/abs/1802.02611v3\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_name: str = \"resnet34\",\n",
    "            encoder_depth: int = 5,\n",
    "            encoder_weights: Optional[str] = \"imagenet\",\n",
    "            encoder_output_stride: int = 16,\n",
    "            decoder_channels: int = 256,\n",
    "            decoder_atrous_rates: tuple = (12, 24, 36),\n",
    "            in_channels: int = 3,\n",
    "            classes: int = 1,\n",
    "            activation: Optional[str] = None,\n",
    "            upsampling: int = 4,\n",
    "            aux_params: Optional[dict] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "        )\n",
    "\n",
    "        if encoder_output_stride == 8:\n",
    "            self.encoder.make_dilated(\n",
    "                stage_list=[4, 5],\n",
    "                dilation_list=[2, 4]\n",
    "            )\n",
    "\n",
    "        elif encoder_output_stride == 16:\n",
    "            self.encoder.make_dilated(\n",
    "                stage_list=[5],\n",
    "                dilation_list=[2]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Encoder output stride should be 8 or 16, got {}\".format(encoder_output_stride)\n",
    "            )\n",
    "\n",
    "        self.decoder = DeepLabV3PlusDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            out_channels=decoder_channels,\n",
    "            atrous_rates=decoder_atrous_rates,\n",
    "            output_stride=encoder_output_stride,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
