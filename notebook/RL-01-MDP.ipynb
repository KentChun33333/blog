{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Key Take Aways </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Continuing task</font> is a task without terminal state ; <font color=blue>Episodes task </font> which each episodes end at terminal state.\n",
    "***\n",
    "<font color=blue>Return</font> is a long-term evaluation of reward-signal\n",
    "\n",
    "<font color=blue>Disconuting return</font> : $ G_{t} = \\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ \\gamma $ is <font color=blue>discounting rate</font> between 0 and 1\n",
    "\n",
    "* $ T $ is the full infinite sequence which as <font color=blue>single epsode sequence</font>\n",
    "\n",
    "* $ k $ represents each step or action\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Markov property</font> : A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\n",
    "\n",
    "*** \n",
    "\n",
    "<font color=blue>Finite MDP </font> is usually defined by its <font color=blue>state</font> and <font color=blue>action sets</font> and by the <font color=blue>one-step dynamics</font> of the environment. \n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> Environment dynamics ( High Level Model ) </font> : \n",
    "\n",
    "$ p( s^{'} \\text{ , } r \\mid s \\text{ , } a) = Pr\\{S_{t+1}=s^{'} \\text{ , } R_{t+1}=r \\mid S_{t}=s \\text{ , } A_{t}=a \\} $\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> Expected reward for state-action pairs </font> : \n",
    "\n",
    "$ r(s \\text{ , } a) = \\mathbb{E}[ R_{t+1} \\mid S_{t}=s \\text{ , } A_{t}=a ] $\n",
    "\n",
    "$r(s \\text{ , } a) = \\sum_{r \\in R} r \\sum_{s^{'} \\in S} p( s^{'}, r \\mid s, a) $\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> State transition probility </font> : \n",
    "\n",
    "$ p( s^{'} \\mid s \\text{ , } a) = Pr\\{ S_{t+1}=s^{'} \\mid S_{t}=s \\text{ , } A_{t}=a  \\} $\n",
    "\n",
    "$ p( s^{'} \\mid s \\text{ , } a) = \\sum_{r \\in R} p( s^{'} \\text{ , } r \\mid s \\text{ , } a) $\n",
    "\n",
    "*** \n",
    "\n",
    "<font color=blue> Expected reward for state-action-nextState </font> : \n",
    "\n",
    "$ r(s,a,s^{'}) = \\mathbb{E}[ R_{t+1} \\mid S_{t}=s \\text{ , } A_{t}=a \\text{ , } S_{t+1}=s^{'} ] $\n",
    "\n",
    "$ r(s,a,s^{'}) = \\frac{\\sum_{r \\in R} r p( s^{'} \\text{ , } r \\mid s \\text{ , } a)}{p( s^{'} \\mid s \\text{ , } a)} $\n",
    "\n",
    "$ r(s,a,s^{'}) = \\frac{\\text{Expected reward for state-action pairs} \\mid S_{t+1}=s^{'}}{\\text{State transition probility}} $\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Finite MDP table </font> : which is often a very large information table \n",
    "<img src=\"image/RL-01-MDP-00.png\" width=350>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> The value function </font> is to estimate the **return under current state (s)** and the value is estimated from experience. The estimation method includes Monte Carlo, DP, TD, Backpropagation, which indicating the deep learning model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Value function under a policy($\\pi$) : </font>\n",
    "\n",
    "$ v_{\\pi}(s) = \\mathbb{E}[G_{t} \\mid S_{t}=s] $\n",
    "\n",
    "$ v_{\\pi}(s) = \\mathbb{E}[\\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} \\mid S_{t}=s ] $\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> **Optimal** Value function : </font>\n",
    "\n",
    "$v_{*}(s) = \\text{max}_{\\pi} v_{\\pi}(s)$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Action-value function under policy($\\pi$) : </font>\n",
    "\n",
    "$ q_{\\pi}(s \\text{ , } a) = \\mathbb{E}[G_{t} \\mid S_{t}=s \\text{ , } A_{t}=a ]$\n",
    "\n",
    "$ q_{\\pi}(s \\text{ , } a) = \\mathbb{E}[\\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} \\mid S_{t}=s \\text{ , } A_{t}=a ]$\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> **Optimal** Action-value function : </font>\n",
    "\n",
    "$ q_{*}(s \\text{ , } a) = \\text{max}_{\\pi} q_{\\pi *}(s \\text{ , } a) $\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Backup Diagram of $v_{\\pi}(s)$ and $q_{\\pi}(s \\text{ , } a)$ :  </font>\n",
    "\n",
    "<img src=\"image/RL-01-MDP-01.png\" width=350>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $v_{\\pi}(s)$ </font> : expresses a relationship between <font color=blue>the value of a state</font> and <font color=blue>the values of its successor states</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $v^{*}_{\\pi}(s)$ </font>\n",
    "\n",
    "$v_{*}(s) = \\text{max } q_{\\pi *} (s,a) $ \n",
    "\n",
    "$v_{*}(s) = \\text{max } \\mathbb{E}_{\\pi *} [ G_{t} \\mid S_{t}=s, A_{t}=a] $ \n",
    "\n",
    "$v_{*}(s) = \\text{max } \\sum_{s^{'},r} p(s^{'},r \\mid s,a)[r+\\gamma v_{*}(s^{'})] $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $q^{*}_{\\pi}(s \\text{ , } a)$ </font>\n",
    "\n",
    "$ q^{*}_{\\pi}(s \\text{ , } a) = \\mathbb{E}[R_{t+1}+\\gamma \\text{max}_{a^{'}}q_{*}(S_{t+1}, a^{'}) \\mid S_{t}=s, A_{t}=a] $\n",
    "\n",
    "\n",
    "$ q^{*}_{\\pi}(s \\text{ , } a) = \\sum_{s^{'},r} p(s^{'},r \\mid s,a) [r+\\gamma \\text{max}_{a^{'}} q_{*}(s^{'},a^{'})] $ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=blue> Backup Diagram of $v_{*}(s)$ and $q_{*}(s \\text{ , } a)$ :  </font>\n",
    "\n",
    "<img src=\"image/RL-01-MDP-02.png\" width=350>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Bellman Equation provide one rout to find the optimal policy </font>\n",
    "\n",
    "This solution relies on 3 assumptions : \n",
    "- Actually know the environmemnt dynamics.\n",
    "- Enough computing resource due to the fact of exhausting search looking for all posibilities.\n",
    "- The Morkov Properties\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many different decision-making methods can be viewed as ways of approximately solving the Bellman optimality equation. \n",
    "\n",
    "For example, heuristic search methods can be viewed as expanding the right-hand side of $v_{*}(s) = \\text{max } \\sum_{s^{'},r} p(s^{'},r \\mid s,a)[r+\\gamma v_{*}(s^{'})] $ several times, up to some depth, forming a “tree” of possibilities, and then using a heuristic evaluation function such as Heuristic search methods or A-star based on the episodic case to approximate.\n",
    "\n",
    "The methods of dynamic programming can be related even more closely to the Bellman optimality equation. \n",
    "\n",
    "Many reinforcement learning methods can be clearly understood as approximately solving the Bellman optimality equation, using actual experienced transitions in place of knowledge of the expected transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this chapter we have become familiar with the basic ideas and algorithms of dynamic programming as they relate to solving finite MDPs. \n",
    "\n",
    "**Policy evaluation** refers to the (typically) iterative computation of the value functions for a given policy. \n",
    "\n",
    "**Policy improvement** refers to the computation of an improved policy given the value function for that policy. \n",
    "\n",
    "Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods. Either of these can be used to reliably compute optimal policies and value functions for finite MDPs given complete knowledge of the MDP.\n",
    "\n",
    "\n",
    "Classical DP methods operate in sweeps through the state set, performing a full backup operation on each state. Each backup updates the value of one state based on the values of all possible successor states and their probabilities of occurring. Full backups are closely related to Bellman equations: they are little more than these equations turned into assignment statements. When the backups no longer result in any changes in value, convergence has occurred to values that satisfy the corre- sponding Bellman equation. Just as there are four primary value functions (v⇡, v⇤, q⇡, and q⇤), there are four corresponding Bellman equations and four correspond- ing full backups. An intuitive view of the operation of backups is given by backup diagrams.\n",
    "Insight into DP methods and, in fact, into almost all reinforcement learning meth- ods, can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea of two interacting processes revolving around an approximate policy and an approximate value function. One process takes the policy as given and performs some form of policy evaluation, changing the value function to be more like the true value function for the policy. The other process takes the value function as given and performs some form of policy improvement, changing the policy to make it bet- ter, assuming that the value function is its value function. Although each process changes the basis for the other, overall they work together to find a joint solution: a policy and value function that are unchanged by either process and, consequently, are optimal. In some cases, GPI can be proved to converge, most notably for the classical DP methods that we have presented in this chapter. In other cases conver-\n",
    "96 CHAPTER 4. DYNAMIC PROGRAMMING gence has not been proved, but still the idea of GPI improves our understanding of\n",
    "the methods.\n",
    "It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that back up states in an arbitrary order, perhaps stochastically determined and using out-of-date infor- mation. Many of these methods can be viewed as fine-grained forms of GPI.\n",
    "Finally, we note one last special property of DP methods. All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea bootstrapping. Many reinforcement learning methods perform bootstrapping, even those that do not require, as DP requires, a complete and accurate model of the environment. In the next chapter we explore reinforcement learning methods that do not require a model and do not bootstrap. In the chapter after that we explore methods that do not require a model but do bootstrap. These key features and properties are separable, yet can be mixed in interesting combinations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
