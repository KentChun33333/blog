{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDPs and Bellman Equations\n",
    "\n",
    "### Learning Goals\n",
    "\n",
    "- Understand the Agent-Environment interface\n",
    "- Understand what MDPs (Markov Decision Processes) are and how to interpret transition diagrams\n",
    "- Understand Value Functions, Action-Value Functions, and Policy Functions\n",
    "- Understand the Bellman Equations and Bellman Optimality Equations for value functions and action-value functions\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Agent & Environment Interface: At each step `t` the agent receives a state `S_t`, performs an action `A_t` and receives a reward `R_{t+1}`. The action is chosen according to a policy function `pi`.\n",
    "- The total return `G_t` is the sum of all rewards starting from time t . Future rewards are discounted at a discount rate `gamma^k`.\n",
    "- Markov property: The environment's response at time `t+1` depends only on the state and action representations at time `t`. The future is independent of the past given the present. Even if an environment doesn't fully satisfy the Markov property we still treat it as if it is and try to construct the state representation to be approximately Markov.\n",
    "- Markov Decision Process (MDP): Defined by a state set S, action set A and one-step dynamics `p(s',r | s,a)`. If we have complete knowledge of the environment we know the transition dynamic. In practice, we often don't know the full MDP (but we know that it's some MDP).\n",
    "- The Value Function `v(s)` estimates how \"good\" it is for an agent to be in a particular state. More formally, it's the expected return `G_t` given that the agent is in state `s`. `v(s) = Ex[G_t | S_t = s]`. Note that the value function is specific to a given policy `pi`.\n",
    "- Action Value function: q(s, a) estimates how \"good\" it is for an agent to be in states and take action a. Similar to the value function, but also considers the action.\n",
    "- The Bellman equation expresses the relationship between the value of a state and the values of its successor states. It can be expressed using a \"backup\" diagram. Bellman equations exist for both the value function and the action value function.\n",
    "- Value functions define an ordering over policies. A policy `p1` is better than `p2` if `v_p1(s) >= v_p2(s)` for all states s. For MDPs, there exist one or more optimal policies that are better than or equal to all other policies.\n",
    "- The optimal state value function `v*(s)` is the value function for the optimal policy. Same for `q*(s, a)`. The Bellman Optimality Equation defines how the optimal value of a state is related to the optimal value of successor states. It has a \"max\" instead of an average.\n",
    "\n",
    "\n",
    "### Lectures & Readings\n",
    "\n",
    "**Required:**\n",
    "\n",
    "- [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf) - Chapter 3: Finite Markov Decision Processes\n",
    "- David Silver's RL Course Lecture 2 - Markov Decision Processes ([video](https://www.youtube.com/watch?v=lfHX2hHRMVQ), [slides](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf))\n",
    "\n",
    "\n",
    "### Exercises\n",
    "\n",
    "This chapter is mostly theory so there are no exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Key Take Aways </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Continuing task</font> is a task without terminal state ; <font color=blue>Episodes task </font> which each episodes end at terminal state.\n",
    "***\n",
    "<font color=blue>Return</font> is a long-term evaluation of reward-signal\n",
    "\n",
    "<font color=blue>Disconuting return</font> : $ G_{t} = \\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ \\gamma $ is <font color=blue>discounting rate</font> between 0 and 1\n",
    "\n",
    "* $ T $ is the full infinite sequence which as <font color=blue>single epsode sequence</font>\n",
    "\n",
    "* $ k $ represents each step or action\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Markov property</font> : A state signal that succeeds in retaining all relevant information is said to be Markov, or to have the Markov property.\n",
    "\n",
    "*** \n",
    "\n",
    "<font color=blue>Finite MDP </font> is usually defined by its <font color=blue>state</font> and <font color=blue>action sets</font> and by the <font color=blue>one-step dynamics</font> of the environment. \n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> Environment dynamics ( High Level Model ) </font> : \n",
    "\n",
    "$ p( s^{'} \\text{ , } r \\mid s \\text{ , } a) = Pr\\{S_{t+1}=s^{'} \\text{ , } R_{t+1}=r \\mid S_{t}=s \\text{ , } A_{t}=a \\} $\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> Expected reward for state-action pairs </font> : \n",
    "\n",
    "$ r(s \\text{ , } a) = \\mathbb{E}[ R_{t+1} \\mid S_{t}=s \\text{ , } A_{t}=a ] $\n",
    "\n",
    "$r(s \\text{ , } a) = \\sum_{r \\in R} r \\sum_{s^{'} \\in S} p( s^{'}, r \\mid s, a) $\n",
    "\n",
    "***\n",
    "\n",
    "<font color=blue> State transition probility </font> : \n",
    "\n",
    "$ p( s^{'} \\mid s \\text{ , } a) = Pr\\{ S_{t+1}=s^{'} \\mid S_{t}=s \\text{ , } A_{t}=a  \\} $\n",
    "\n",
    "$ p( s^{'} \\mid s \\text{ , } a) = \\sum_{r \\in R} p( s^{'} \\text{ , } r \\mid s \\text{ , } a) $\n",
    "\n",
    "*** \n",
    "\n",
    "<font color=blue> Expected reward for state-action-nextState </font> : \n",
    "\n",
    "$ r(s,a,s^{'}) = \\mathbb{E}[ R_{t+1} \\mid S_{t}=s \\text{ , } A_{t}=a \\text{ , } S_{t+1}=s^{'} ] $\n",
    "\n",
    "$ r(s,a,s^{'}) = \\frac{\\sum_{r \\in R} r p( s^{'} \\text{ , } r \\mid s \\text{ , } a)}{p( s^{'} \\mid s \\text{ , } a)} $\n",
    "\n",
    "$ r(s,a,s^{'}) = \\frac{\\text{Expected reward for state-action pairs} \\mid S_{t+1}=s^{'}}{\\text{State transition probility}} $\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Finite MDP table </font> : which is often a very large information table \n",
    "<img src=\"image/RL-01-MDP-00.png\" width=350>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> The value function </font> is to estimate the **return under current state (s)** and the value is estimated from experience. The estimation method includes Monte Carlo, DP, TD, Backpropagation, which indicating the deep learning model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Value function under a policy($\\pi$) : </font>\n",
    "\n",
    "$ v_{\\pi}(s) = \\mathbb{E}[G_{t} \\mid S_{t}=s] $\n",
    "\n",
    "$ v_{\\pi}(s) = \\mathbb{E}[\\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} \\mid S_{t}=s ] $\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Action-value function under policy($\\pi$) : </font>\n",
    "\n",
    "$ q_{\\pi}(s \\text{ , } a) = \\mathbb{E}[G_{t} \\mid S_{t}=s \\text{ , } A_{t}=a ]$\n",
    "\n",
    "$ q_{\\pi}(s \\text{ , } a) = \\mathbb{E}[\\sum^{T-t-1}_{k=0} \\gamma^{k}R_{t+k+1} \\mid S_{t}=s \\text{ , } A_{t}=a ]$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> Backup Diagram of $v_{\\pi}(s)$ and $q_{\\pi}(s \\text{ , } a)$ :  </font>\n",
    "\n",
    "<img src=\"image/RL-01-MDP-01.png\" width=350>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $v_{\\pi}(s)$ </font> : expresses a relationship between <font color=blue>the value of a state</font> and <font color=blue>the values of its successor states</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> The Optimal value function : </font> There is always at least one policy that is better than or equal to all other policies. This is an optimal policy ($\\pi^{*}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $v^{*}_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>Bellman equation for $q^{*}_{\\pi}(s \\text{ , } a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
