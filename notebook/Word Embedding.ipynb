{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Walk through Word Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:#DF7E22\"> Core Concept : </span>\n",
    "\n",
    "- Embedding is to transformn from the **<span style=\"color:blue\">Sparse</span>** representations to a **<span style=\"color:blue\">Higher density</span>** representations that contain **<span style=\"color:blue\">More solid context-information</span>** in a **<span style=\"color:blue\">Vectors space</span>**. \n",
    "\n",
    "- The original ideal is to solve the sparse representation that we used one-hot vector to represent each word in the Natural Language Process (NPL-domain)\n",
    "\n",
    "- This tech is a kind of alternative with **<span style=\"color:blue\">Autoencoder</span>** \n",
    "\n",
    "- Due the Embedding learning problem usually a **<span style=\"color:blue\">Unnormalized problem</span>**, it requires lots of computation. We actually have variable approximation-way to interpret it and implement it. \n",
    "\n",
    "- In this post, I am going to take a walk though tensorflow implementation, which is base on this **[Paper](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Key Words :</span>\n",
    "\n",
    "- Continuous Bag of Words (CBOW)\n",
    "- The Skip-Gram \n",
    "- Hierachical Softmax : For Fast Computer with O[log(M)] where M is the population/vocabulary \n",
    " - Binary Tree \n",
    "- Noise Contrastive Estimation (An alternative to the Hierachical Softmax) \n",
    " - NCE loss ( learn by comparison ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Noise-contrastive estimation](http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)\n",
    "We train our models using noise-contrastive estimation, a method for fitting unnormalized models, adapted to neural language modelling in [14]. NCE is based on the reduction of density estimation\n",
    "to probabilistic binary classification. The basic idea is to train a logistic regression classifier to\n",
    "discriminate between samples from the data distribution and samples from some “noise” distribution,\n",
    "based on the ratio of probabilities of the sample under the model and the noise distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Math Interpretation : </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **<span style=\"color:blue\"> Neural Probabilistic Language Models </span>**\n",
    "\n",
    " - $ P_{\\theta}^{h}(w) = \\frac{exp(s_{\\theta}(w, h))}{\\sum_{\\hat{w}} exp(s_{\\theta}(w, h))}$\n",
    "\n",
    "\n",
    "- **<span style=\"color:blue\"> Scalable Log-bilinear Models </span>**\n",
    "\n",
    " - $ \\hat{q}(h) = \\sum_{i=1}^{n} c_{i} \\odot r_{w_{i}} $ \n",
    " - $c_{i} $ : weight vector for the context word in position i \n",
    " - $r_{w_{i}} $ : context word\n",
    " - $ \\odot $ denotes element-wise multiplication. \n",
    " - The context can consist of words preceding, following, or surrounding the word being predicted. \n",
    " \n",
    " \n",
    "- **<span style=\"color:blue\"> The Scoring Function </span>**\n",
    " - $ s_{\\theta} (w,h) = \\hat{q}(h)^\\intercal q_{w} + b_{w}$\n",
    " - which is to compute the similarity between the predicted feature vector and one for word w\n",
    " - $ \\hat{q}(h) $ is the predict word.\n",
    " - $ q_{w} $ is the targeted word.\n",
    "\n",
    "- Predicting an n-word context requires modelling the joint distribution of n words\n",
    " - $ P^{w}_{\\theta}(h) = \\Pi_{i=1}^{n}P_{i, \\theta}^{w}(w_{i})$\n",
    " \n",
    "- The context word distributions $ P_{i, \\theta}^{w}(w_{i}) $ are simply vLBL models that condition on the current word\n",
    " - $ s_{i, \\theta}(w_{i}, w) = (c_{i} \\odot r_{w_{i}})^\\intercal q_{w_{i}} + b_{w_{i}}  $\n",
    " \n",
    "- **<span style=\"color:blue\"> Simpler version that without position-dependent weights, defined by the scoring function </span>**\n",
    " - $ s_{i, \\theta} (w_{i},w) = r_{w}^\\intercal q_{w_{i}} + b_{w_{i}}$ \n",
    " - The resulting model is the non-hierarchical counterpart of the Skip-gram model\n",
    " \n",
    "- **<span style=\"color:blue\"> Noise Constrastive estimation </span>**\n",
    " - Fitting unnormalized models\n",
    " - NCE is based on the reduction of density estimation to probabilistic binary classification.\n",
    " - Train a logistic regression classifier to discriminate between samples from the data distribution and samples from some “noise” distribution, based on the ratio of probabilities of the sample under the model and the noise distribution\n",
    " - Suppose we would like to learn the distribution of words for some specific context h, denoted by $P_{h}(w)$. \n",
    " - Treating the training data as positive examples and samples from a noise distribution $P_{n}(w)$ as negative examples.\n",
    " - Assume noise samples are k times more frequent than data samples\n",
    " - Use the (global) unigram distribution of the training data as the noise distribution.\n",
    " - $ P^{h}(D=1 \\mid w,\\theta) = \\frac{P^{h}_{\\theta}(w)}{P^{h}_{\\theta}(w)+ kP_{n}(w)} = \\sigma (\\nabla s_{\\theta}(w,h)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Code Implementation : </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Reference : </span>\n",
    "\n",
    "- [A nice blog : Word-Embbeding and Autoencoder](https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.rcrh3ybeb)\n",
    "\n",
    "- [Tensorflow tutorials](https://www.tensorflow.org/versions/r0.12/tutorials/word2vec/index.html)\n",
    "\n",
    "- [TF tutirial in chinese](http://www.jeyzhang.com/tensorflow-learning-notes-3.html)\n",
    "\n",
    "- [A nice script that implement embedding with Skip-gram and COBV with hierachical softmax](https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
