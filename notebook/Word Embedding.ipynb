{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding : A walk through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:#DF7E22\"> Core Concept : </span>\n",
    "\n",
    "- Embedding is to transformn from the **<span style=\"color:blue\">Sparse</span>** representations to a **<span style=\"color:blue\">Higher density</span>** representations that contain **<span style=\"color:blue\">More solid context-information</span>** in a **<span style=\"color:blue\">Vectors space</span>**. \n",
    "\n",
    "- The original ideal is to solve the sparse representation that we used one-hot vector to represent each word in the Natural Language Process (NPL-domain)\n",
    "\n",
    "- This tech is a kind of alternative with **<span style=\"color:blue\">Autoencoder</span>** \n",
    "\n",
    "- Due the Embedding learning problem usually a **<span style=\"color:blue\">Unnormalized problem</span>**, it requires lots of computation. We actually have variable approximation-way to interpret it and implement it. \n",
    "\n",
    "- In this post, I am going to take a walk though tensorflow implementation, which is base on this **[Paper](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Key Words :</span>\n",
    "\n",
    "- Continuous Bag of Words (CBOW)\n",
    "- The Skip-Gram \n",
    "- Hierachical Softmax : For Fast Computer with O[log(M)] where M is the population/vocabulary \n",
    " - Binary Tree \n",
    "- Noise Contrastive Estimation (An alternative to the Hierachical Softmax) \n",
    " - NCE loss ( learn by comparison ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Noise-contrastive estimation](http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf)\n",
    "We train our models using noise-contrastive estimation, a method for fitting unnormalized models, adapted to neural language modelling in [14]. NCE is based on the reduction of density estimation\n",
    "to probabilistic binary classification. The basic idea is to train a logistic regression classifier to\n",
    "discriminate between samples from the data distribution and samples from some “noise” distribution,\n",
    "based on the ratio of probabilities of the sample under the model and the noise distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Math Interpretation : </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **<span style=\"color:blue\"> Neural Probabilistic Language Models </span>**\n",
    "\n",
    " - $ P_{\\theta}^{h}(w) = \\frac{exp(s_{\\theta}(w, h))}{\\sum_{\\hat{w}} exp(s_{\\theta}(w, h))}$\n",
    "\n",
    "\n",
    "- **<span style=\"color:blue\"> Scalable Log-bilinear Models </span>**\n",
    "\n",
    " - $ \\hat{q}(h) = \\sum_{i=1}^{n} c_{i} \\odot r_{w_{i}} $ \n",
    " - $c_{i} $ : weight vector for the context word in position i \n",
    " - $ \\odot $ denotes element-wise multiplication. \n",
    " - The context can consist of words preceding, following, or surrounding the word being predicted. \n",
    " \n",
    " \n",
    "- **<span style=\"color:blue\"> The Scoring Function </span>**\n",
    " - $ s_{\\theta} (w,h) = \\hat{q}(h)^\\intercal q_{w} + b_{w}$\n",
    " - which is to compute the similarity between the predicted feature vector and one for word w:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Code Implementation : </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style = \"color:#DF7E22\">Reference : </span>\n",
    "\n",
    "- [A nice blog : Word-Embbeding and Autoencoder](https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a#.rcrh3ybeb)\n",
    "\n",
    "- [Tensorflow tutorials](https://www.tensorflow.org/versions/r0.12/tutorials/word2vec/index.html)\n",
    "\n",
    "- [TF tutirial in chinese](http://www.jeyzhang.com/tensorflow-learning-notes-3.html)\n",
    "\n",
    "- [A nice script that implement embedding with Skip-gram and COBV with hierachical softmax](https://github.com/deborausujono/word2vecpy/blob/master/word2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
