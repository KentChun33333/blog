{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "**Batch normalization** is a popular method to fasten deep-network training process also solving the gradient vanishing or exploding problem. In [this post](https://kentchun33333.github.io/), I am going to first discuss some ideas, then take a glance on algorithms in the [paper](https://arxiv.org/pdf/1502.03167v3.pdf),and finally take a deep look on how tensorflow implementation it. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Concepts about Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite common to use normalization in neural network, especially deep convnet. Activation function is working like an filtering/magnification in feature signals, and on the other hand, the various normalization method is working like an smoothing/de-amplifier. The underlying concept is that ** only difference matters while delievering information**. Just think the information like electricity in cpu, nowadays the driving voltage is much lower than the past cpu, but it carried more heavy work, compute even faster and spend less energy.\n",
    "\n",
    "### 2. How to Chose Normalization Method \n",
    "**There are multiple ways to normalize our data**. Before batch normalization layer, there are several ways/methods like as :\n",
    "- per-image-normalize\n",
    "- per-image whitening, \n",
    "- per-batch normalization,\n",
    "- per-batch whitening,\n",
    "- local constrast normalization (LCN) \n",
    "- local response normalization (LRN)...etc \n",
    "- batch normalization \n",
    "- layer normalization\n",
    "- instance normalization \n",
    "- group normalization\n",
    "\n",
    "** The strategy to add what normalization is correlated on the operation before/after it.** \n",
    "\n",
    "For example, if you are going to applied a RELU with a threshod equal to 0.5 ( which is not common) after a normalization-layer, you probabily dont want to ouput of this normalization-layer to be at range between 0,1. Since the following RELU-layer would swipe too many information... \n",
    "\n",
    "For another example, you would like to introduce a **depth-wise or channel-wise normalization** after the conv-layer. Because, the conv-layer is actually a depth-wish operation. \n",
    "\n",
    "For more informations about normalization, check this [post](http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/).\n",
    "\n",
    "### 3. Math Expression of Batch Normalization\n",
    "\n",
    "\n",
    "There are two different operations in Batch Normalization.\n",
    "\n",
    "- Training: to calculate mini batch mean in order to normalize the batch\n",
    "\n",
    "- Inference: apply pre-calculated mini batch statistics\n",
    "  - (To calculate this mini batch statics, we using moving average)\n",
    "  - running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "  - running_var = momentum * running_var + (1 - momentum) * sample_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 1: Batch Normalization within Batch} \\\\  \n",
    "\\hline \n",
    "\\text{Input: Values of x over a mini-Batch : } B \\{  x_{1 \\text{ ... m }} \\}  \\\\\n",
    "\\text{Parameters: } \\beta \\text{ and } \\gamma \\\\\n",
    "\\text{Output: A set of } Y : \\{ y_{i} = \\text{ BatchNorm}_{\\beta, \\gamma}(x^{i}) \\} \\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i}) \\text{ ----------------- mini-batch mean}\\\\\n",
    "\\text{ } \\alpha^{2}_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i} - \\mu_{\\beta})^{2} \\text{ --------- mini-batch variance}\\\\\n",
    "\\text{ } \\hat{x}_{i} \\leftarrow \\frac{x_{i} - \\mu_{\\beta}}{\\sqrt{\\alpha^{2}_{\\beta}+ \\epsilon}} \\text{ ---------------------- normalization where epsilon is the number to prevent dividing zero } \\\\\n",
    "\\text{ } y_{i} \\leftarrow \\gamma \\hat{x}_{i} + \\beta = \\text{BN}_{\\gamma, \\beta}(x_{i}) \\text{ -------- scale and shift}\\\\\n",
    "\\end{array} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 2: Training with Batch-Normalized } \\\\  \n",
    "\\hline \n",
    "\\text{Input : Network N with trainable parameters } \\Theta \\text{ ; subset of activations } \\{ x^{(k)}\\}^{K}_{k=1} \\\\\n",
    "\\text{Output : Batch-normalized network for inference, N}^{inf}_{\\text{BN}} \\\\\n",
    "\\text{ - N}^{\\text{tr}}_{\\text{BN}} \\leftarrow \\text{ N  (Training BN network)} \\\\\n",
    "\\text{ - } \\textbf{for } k = 1 ... K \\textbf{  do } : \\\\ \n",
    "\\text{ - Add transformation } y^{(k)} = \\text{BN}_{\\gamma^{(k)}, \\beta^{(k)}}(x^{(k)}) \\text{ to N}^{\\text{tr}}_{\\text{BN}} \\text{ ( Alg. 1 )} \\\\ \n",
    "\\text{ - Modify each layer in N}^{\\text{tr}}_{\\text{BN}} \\text{ with input } x^{(k)} \\text{ to take } y^{(k)} \\text{ instead} \\\\\n",
    "\\text{ - } \\textbf{end for }\\\\\n",
    "\\\\\n",
    "\\text{ - Train N}^{\\text{tr}}_{\\text{BN}} \\text{ to optimize the parameters } \\Theta \\cup \\{ \\gamma^{(k)}, \\beta^{(k)} \\}^{K}_{k=1}\\\\\n",
    "\\end{array} $\n",
    "\n",
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 3: Inference with Batch-Normalized} \\\\  \n",
    "\\hline \n",
    "\\text{ - N}^{\\text{inf}}_{\\text{BN}} \\leftarrow \\text{N}^{\\text{tr}}_{\\text{BN}} \\text{ (inference BN network with forzen parameters)} \\\\\n",
    "\\\\\n",
    "\\text{ - } \\textbf{for } k = 1 ... K \\textbf{  do } : \\\\ \n",
    "\\text{ - //For clarity} x=x^{(k)}, \\gamma = \\gamma^{(k)}, \\beta = \\beta^{(k)} ... etc \\\\\n",
    "\\text{ - Process multiple training mini-batches B, each of size m, and average over them :}\\\\\n",
    "\\text{ - E}[x] \\leftarrow \\text{E}_{\\beta}[\\mu_{\\beta}]\\\\\n",
    "\\text{ - Var}[x] \\leftarrow \\frac{m}{m-1} \\text{E}_{\\beta}[\\alpha^{2}_{\\beta}]\\\\\n",
    "\\text{ - In N}^{\\text{inf}}_{\\text{BN}} \\text{, replace the transform } y = \\text{BN}_{\\gamma, \\beta}(x) \\text{ with } y = \\frac{\\gamma}{\\sqrt{\\text{Var}[x]+\\epsilon}} \\dot x + (\\beta - \\frac{\\gamma \\text{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}})\\\\\n",
    "\\text{ - } \\textbf{end for }\\\\\n",
    "\\end{array} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tensorflow Implementations\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def batch_norm(x, phase_train, scope='bn', affine=True):\n",
    "    \"\"\"\n",
    "    Batch normalization on convolutional maps.\n",
    "    from: https://stackoverflow.com/questions/33949786/how-could-i-\n",
    "    use-batch-normalization-in-tensorflow\n",
    "    Only modified to infer shape from input tensor x.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x\n",
    "        Tensor, 4D BHWD input maps\n",
    "    phase_train\n",
    "        boolean tf.Variable, true indicates training phase\n",
    "    scope\n",
    "        string, variable scope\n",
    "    affine\n",
    "        whether to affine-transform outputs\n",
    "    Return\n",
    "    ------\n",
    "    normed\n",
    "        batch-normalized maps\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        shape = x.get_shape().as_list()\n",
    "\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[shape[-1]]),\n",
    "                           name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[shape[-1]]),\n",
    "                            name='gamma', trainable=affine)\n",
    "\n",
    "        batch_mean, batch_var = tf.nn.moments(x, [0, 1, 2], name='moments')\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "        ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "\n",
    "        def mean_var_with_update():\n",
    "            \"\"\"Summary\n",
    "            Returns\n",
    "            -------\n",
    "            name : TYPE\n",
    "                Description\n",
    "            \"\"\"\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        mean, var = tf.cond(phase_train,\n",
    "                                          mean_var_with_update,\n",
    "                                          lambda: (ema_mean, ema_var))\n",
    "\n",
    "        normed = tf.nn.batch_norm_with_global_normalization(\n",
    "            x, mean, var, beta, gamma, 1e-3, affine)\n",
    "    return normed\n",
    "    \n",
    "# this is the function of tf.nn.batch_normalization\n",
    "def batch_normalization(x,\n",
    "                        mean,\n",
    "                        variance,\n",
    "                        offset,\n",
    "                        scale,\n",
    "                        variance_epsilon,\n",
    "                        name=None):\n",
    "  with ops.name_scope(name, \"batchnorm\", [x, mean, variance, scale, offset]):\n",
    "    inv = math_ops.rsqrt(variance + variance_epsilon)\n",
    "    if scale is not None:\n",
    "      inv *= scale\n",
    "    return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n",
    "        offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "\n",
    "- http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/\n",
    "\n",
    "- https://github.com/pkmital/CADL/blob/master/session-4/libs/batch_norm.py\n",
    "\n",
    "- http://cthorey.github.io./backpropagation/\n",
    "\n",
    "- http://r2rt.com/implementing-batch-normalization-in-tensorflow.html\n",
    "\n",
    "- https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n",
    "\n",
    "- https://www.zhihu.com/question/38102762\n",
    "\n",
    "- http://shuokay.com/2016/10/15/wavenet/\n",
    "\n",
    "- http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html\n",
    "\n",
    "- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n",
    "\n",
    "- https://github.com/leichaocn/normalization_of_neural_network/blob/master/batch_normalization_practice.ipynb"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
