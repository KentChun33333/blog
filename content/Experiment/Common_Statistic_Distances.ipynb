{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Distance that used machine learning \n",
    "\n",
    "There are many distances used in machine learning. Following list several of most common-used distance.\n",
    "\n",
    "##### 1. Euclidean distance \n",
    "\n",
    " - $ \\sqrt{ \\sum_{t=1}^{p} (x_{it} - x_{jt})^{2} }$\n",
    "\n",
    "##### 2. [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance)\n",
    "\n",
    " - $  ( \\sum_{i=1}^{n} \\| x_{i} - y_{i} \\|^{p} )^{1/p} $\n",
    "\n",
    "##### 3. Cosine distance\n",
    "\n",
    " - $  \\frac{\\sum_{1}^{n} ( A{i} \\text{ x } B_{i} ) }{ \\sqrt{ \\sum_{1}^{n} A{i}^{2}} \\text{ x } \\sqrt{ \\sum_{1}^{n} B{i}^{2} } } $\n",
    "\n",
    "##### 4. [Hotelling T2 distance](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution)\n",
    "\n",
    " - Mahalanobis distance is a measure of distance between a point and distribution. So if we want to check if a point belongs to a particular distribution or not, we can use Hotelling's T-test, which is squared Mahalanobis distance. But if we have two sample distribution and we want to check if they belong to the same group or not, we can use two sample Hotelling's T-test, that is,\n",
    "\n",
    "##### 5. Mahalanobis Distance \n",
    "\n",
    " - Computes the Mahalanobis distance between the points. The Mahalanobis distance between two points $ u $ and $ v $ is  $ \\sqrt{(u-v)(1/V)(u-v)^{T}} $ where (the VI variable) is the inverse covariance. If VI is not None, VI will be used as the inverse covariance matrix.\n",
    "\n",
    " - **Limitation** : A Mahalanobis distance requires a covariance matrix. A NON-singular covariance matrix. If your matrix is singular, then the computation will produce garbage, since you cannot invert a singular matrix. Since you don't have sufficient data to estimate a complete covariance matrix, mahal must fail.\n",
    "\n",
    "##### 6. Seuclidean Distance \n",
    "\n",
    " - Computes the standardized Euclidean distance. The standardized Euclidean distance between two n-vectors $u$ and $v$ is\n",
    "  $ \\sqrt{ \\sum{(u_{i}-v_{i})^{2}} / (V[X_{i}]) }  $\n",
    "- V is the variance vector; V[i] is the variance computed over all the iâ€™th components of the points. If not passed, it is automatically computed.\n",
    "\n",
    "##### 7. Edit Distance \n",
    "\n",
    "##### 8. Shape-Based Distance \n",
    "\n",
    "##### 9. KD distance \n",
    "##### 10. .....etc \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
