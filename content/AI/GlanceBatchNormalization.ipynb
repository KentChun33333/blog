{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "**Batch normalization** is a popular method to fasten deep-network training process also solving the gradient vanishing or exploding problem. In this [post](https://kentchun33333.github.io/), I am going to first discuss some ideas, then take a glance at math expression on algorithms in original [paper](https://arxiv.org/pdf/1502.03167v3.pdf),and finally take a deep look on how to implement it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Brief concepts about normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite common to use normalization in neural network, especially deep convnet. Before batch normalization layer, there are several ways/methods like as per-image-normalize, per-image whitening, per-batch normalization, per-batch whitening, local constrast normalization (LCN) and local response normalization (LRN). These methods are actually really good, and is sucessfully working in some conditions.\n",
    "\n",
    "To me, the main core ideal is simple, that is ** only difference matters while delievering information**. Just think the information like electricity in cpu, nowadays the driving voltage is much lower than the past cpu, but it carried more heavy work, compute even faster and spend less energy. \n",
    "\n",
    "**We actually have multiple ways to normalize our data before or after any operation.** A good way or strategy to add the normalization is actually **depending on the operation before or after it.** For example, if you are going to applied a RELU with a threshod equal to 0.5 ( which is not common) and you would like to add a normalization-method before it. You probabily do not want to only add the normalization but also some shift like 0.1 or 0.3 to make sure this RELU-layer do not swipe too many information. For another example, you would like to introduce a **depth-wise or channel-wise normalization** after the conv-layer. Because, the conv-layer is actually a depth-wish operation. Depth in conv-operation is actually the numbers of filters/kernels which coressponding to a **( **filter-width \\* filter-height \\* previous-depth** )** of weights/neurals.\n",
    "\n",
    "The batch normalization is basically following this concept but adding a trainable feature to it. **This feature makes it beautiful.**\n",
    "\n",
    "For more informations about normalization, check this [post](http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Expression in Math "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 1 : Batch Normalizing Transform, applied to activation x over a mini-batch. } \\\\  \n",
    "\\hline \n",
    "\\text{Input : Values of x over a mini-Batch : } B \\{  x_{1 \\text{ ... m }} \\} \\text{ ; Parameters to be learned : } \\beta \\text{ and } \\gamma \\\\\n",
    "\\text{Output : A set of } Y : \\{ y_{i} = \\text{ BatchNorm}_{\\beta, \\gamma}(x^{i}) \\} \\\\\n",
    "\\text{ } \\mu_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i}) \\text{ ----------------- mini-batch mean}\\\\\n",
    "\\text{ } \\alpha^{2}_{ \\beta } \\leftarrow  \\frac{1}{m} \\sum_{i=1}^m (x_{i} - \\mu_{\\beta})^{2} \\text{ --------- mini-batch variance}\\\\\n",
    "\\text{ } \\hat{x}_{i} \\leftarrow \\frac{x_{i} - \\mu_{\\beta}}{\\sqrt{\\alpha^{2}_{\\beta}+ \\epsilon}} \\text{ ---------------------- normalization where epsilon is the number to prevent dividing zero } \\\\\n",
    "\\text{ } y_{i} \\leftarrow \\gamma \\hat{x}_{i} + \\beta = \\text{BN}_{\\gamma, \\beta}(x_{i}) \\text{ -------- scale and shift}\\\\\n",
    "\\end{array} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ \\begin{array} \\\\\n",
    "\\text{Algorithm 2 : Training a Batch-Normalized Network } \\\\  \n",
    "\\hline \n",
    "\\text{Input : Network N with trainable parameters } \\Theta \\text{ ; subset of activations } \\{ x^{(k)}\\}^{K}_{k=1} \\\\\n",
    "\\text{Output : Batch-normalized network for inference, N}^{inf}_{\\text{BN}} \\\\\n",
    "\\text{ 1: N}^{\\text{tr}}_{\\text{BN}} \\leftarrow \\text{ N ------ Training BN network} \\\\\n",
    "\\text{ 2: } \\textbf{for } k = 1 ... K \\textbf{  do } : \\\\ \n",
    "\\text{ 3: Add transformation } y^{(k)} = \\text{BN}_{\\gamma^{(k)}, \\beta^{(k)}}(x^{(k)}) \\text{ to N}^{\\text{tr}}_{\\text{BN}} \\text{ ( Alg. 1 )} \\\\ \n",
    "\\text{ 4: Modify each layer in N}^{\\text{tr}}_{\\text{BN}} \\text{ with input } x^{(k)} \\text{ to take } y^{(k)} \\text{ instead} \\\\\n",
    "\\text{ 5: } \\textbf{end for }\\\\\n",
    "\\\\\n",
    "\\text{ 6: Train N}^{\\text{tr}}_{\\text{BN}} \\text{ to optimize the parameters } \\Theta \\cup \\{ \\gamma^{(k)}, \\beta^{(k)} \\}^{K}_{k=1}\\\\\n",
    "\\text{ 7: N}^{\\text{inf}}_{\\text{BN}} \\leftarrow \\text{N}^{\\text{tr}}_{\\text{BN}} text{ ---inference BN network with forzen parameters} \\\\\n",
    "\\\\\n",
    "\\text{ 8: } \\textbf{for } k = 1 ... K \\textbf{  do } : \\\\ \n",
    "\\text{ 9: //For clarity} x=x^{(k)}, \\gamma = \\gamma^{(k)}, \\beta = \\beta^{(k)} ... etc \\\\\n",
    "\\text{10: Process multiple training mini-batches B, each of size m, and average over them :}\\\\\n",
    "\\text{10: E}[x] \\leftarrow \\text{E}_{\\beta}[\\mu_{\\beta}]\\\\\n",
    "\\text{10: Var}[x] \\leftarrow \\frac{m}{m-1} \\text{E}_{\\beta}[\\alpha^{2}_{\\beta}]\\\\\n",
    "\\text{11: In N}^{\\text{inf}}_{\\text{BN}} \\text{, replace the transform } y = \\text{BN}_{\\gamma, \\beta}(x) \\text{ with } y = \\frac{\\gamma}{\\sqrt{\\text{Var}[x]+\\epsilon}} \\dot x + (\\beta - \\frac{\\gamma \\text{E}[x]}{\\sqrt{\\text{Var}[x]+\\epsilon}})\\\\\n",
    "\\text{12: } \\textbf{end for }\\\\\n",
    "\\end{array} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Something you should know before diving into code.\n",
    "\n",
    "To implement batch normalization is, in fact, really simple, as long as, you know the following things : \n",
    "\n",
    "#### 3-1 Automatic Differentiation : \n",
    "\n",
    "There are basically 4 main methods to compute derivatives or to say **backpropagation**.\n",
    "\n",
    "- (1) Manually working out derivatives and coding the result\n",
    "\n",
    "- (2) Numerical differentiation (using finite difference approximations)\n",
    "\n",
    "- (3) **Symbolic differentiation** (expression manipulation in SW such as Max, Mathematica & Maple)\n",
    "\n",
    "- (4) **Automatic differentiation** (Forward, Reversed & Optical Jocobian Accumulation)\n",
    "\n",
    "And most of popular deep learning library had implemented either automatic reverse differenciation or symbolic differenciation while computing gradients and Hessian of an objective fuction.  **Therefore, while implement the batch-normalization layer in real production, we just need to focus on the forward propagation.** (Alg-1)\n",
    "\n",
    "For more information about automatic differentiation, check this [paper](https://arxiv.org/pdf/1502.05767v2.pdf).\n",
    "\n",
    "#### 3-2 Moving Average & Variance in mini-batch traning \n",
    "\n",
    "Since we usually use mini-batch traning, **the concept of estimation the population average and variance by moving-methods** is introduced while the batch-normalization is in training state. It just makes an approximation of the suitable mean & variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch-normalization in Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is from [tf-contrib-layer.](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py) and the test-case could be found in [a stackoverflow-issue.](http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@add_arg_scope\n",
    "def batch_norm(inputs,\n",
    "               decay=0.999,\n",
    "               center=True,\n",
    "               scale=False,\n",
    "               epsilon=0.001,\n",
    "               activation_fn=None,\n",
    "               updates_collections=ops.GraphKeys.UPDATE_OPS,\n",
    "               is_training=True,\n",
    "               reuse=None,\n",
    "               variables_collections=None,\n",
    "               outputs_collections=None,\n",
    "               trainable=True,\n",
    "               scope=None):\n",
    "  \"\"\"Adds a Batch Normalization layer from http://arxiv.org/abs/1502.03167.\n",
    "    \"Batch Normalization: Accelerating Deep Network Training by Reducing\n",
    "    Internal Covariate Shift\"\n",
    "    Sergey Ioffe, Christian Szegedy\n",
    "  Can be used as a normalizer function for conv2d and fully_connected.\n",
    "  Args:\n",
    "    inputs: a tensor of size `[batch_size, height, width, channels]`\n",
    "            or `[batch_size, channels]`.\n",
    "    decay: decay for the moving average.\n",
    "    center: If True, subtract `beta`. If False, `beta` is ignored.\n",
    "    scale: If True, multiply by `gamma`. If False, `gamma` is\n",
    "      not used. When the next layer is linear (also e.g. `nn.relu`), this can be\n",
    "      disabled since the scaling can be done by the next layer.\n",
    "    epsilon: small float added to variance to avoid dividing by zero.\n",
    "    activation_fn: Optional activation function.\n",
    "    updates_collections: collections to collect the update ops for computation.\n",
    "      If None, a control dependency would be added to make sure the updates are\n",
    "      computed.\n",
    "    is_training: whether or not the layer is in training mode. In training mode\n",
    "      it would accumulate the statistics of the moments into `moving_mean` and\n",
    "      `moving_variance` using an exponential moving average with the given\n",
    "      `decay`. When it is not in training mode then it would use the values of\n",
    "      the `moving_mean` and the `moving_variance`.\n",
    "    reuse: whether or not the layer and its variables should be reused. To be\n",
    "      able to reuse the layer scope must be given.\n",
    "    variables_collections: optional collections for the variables.\n",
    "    outputs_collections: collections to add the outputs.\n",
    "    trainable: If `True` also add variables to the graph collection\n",
    "      `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n",
    "    scope: Optional scope for `variable_op_scope`.\n",
    "  Returns:\n",
    "    a tensor representing the output of the operation.\n",
    "  \"\"\"\n",
    "  # =========================================================================\n",
    "  # This variable_op_scope is actually depreciate in r1.1 of TF\n",
    "  # It basically provide an context-manager for variable_ops\n",
    "  # \n",
    "  with variable_scope.variable_op_scope([inputs],\n",
    "                                        scope, 'BatchNorm', reuse=reuse) as sc:\n",
    "    inputs_shape = inputs.get_shape()\n",
    "    dtype = inputs.dtype.base_dtype\n",
    "    \n",
    "    # =======================================================================\n",
    "    # Base on channel : axis = list(range(len(inputs_shape) - 1)) \n",
    "    # -----------------------\n",
    "    # However, it is suggested that to use axis as [0,1,2] to tf.nn.moments \n",
    "    # \n",
    "    axis = list(range(len(inputs_shape) - 1)) \n",
    "    params_shape = inputs_shape[-1:]\n",
    "    # Allocate parameters for the beta and gamma of the normalization.\n",
    "    beta, gamma = None, None\n",
    "    \n",
    "    \n",
    "    # =======================================================================\n",
    "    # init the variable according to the mode/state/parameters\n",
    "    \n",
    "    if center:\n",
    "      beta_collections = utils.get_variable_collections(variables_collections,\n",
    "                                                        'beta')\n",
    "      beta = variables.model_variable('beta',\n",
    "                                      shape=params_shape,\n",
    "                                      dtype=dtype,\n",
    "                                      initializer=init_ops.zeros_initializer,\n",
    "                                      collections=beta_collections,\n",
    "                                      trainable=trainable)\n",
    "    if scale:\n",
    "      gamma_collections = utils.get_variable_collections(variables_collections,\n",
    "                                                         'gamma')\n",
    "      gamma = variables.model_variable('gamma',\n",
    "                                       shape=params_shape,\n",
    "                                       dtype=dtype,\n",
    "                                       initializer=init_ops.ones_initializer,\n",
    "                                       collections=gamma_collections,\n",
    "                                       trainable=trainable)\n",
    "    # =======================================================================\n",
    "    # Somve moving method \n",
    "    # ---------------------\n",
    "    # Create moving_mean and moving_variance variables and add them to the\n",
    "    # appropiate collections.\n",
    "    \n",
    "    moving_mean_collections = utils.get_variable_collections(\n",
    "        variables_collections, 'moving_mean')\n",
    "    moving_mean = variables.model_variable(\n",
    "        'moving_mean',\n",
    "        shape=params_shape,\n",
    "        dtype=dtype,\n",
    "        initializer=init_ops.zeros_initializer,\n",
    "        trainable=False,\n",
    "        collections=moving_mean_collections)\n",
    "    moving_variance_collections = utils.get_variable_collections(\n",
    "        variables_collections, 'moving_variance')\n",
    "    moving_variance = variables.model_variable(\n",
    "        'moving_variance',\n",
    "        shape=params_shape,\n",
    "        dtype=dtype,\n",
    "        initializer=init_ops.ones_initializer,\n",
    "        trainable=False,\n",
    "        collections=moving_variance_collections)\n",
    "        \n",
    "        \n",
    "    # =======================================================================\n",
    "    # mentioned above \n",
    "    \n",
    "    if is_training:\n",
    "      # =======================================================================\n",
    "      # Calculate the moments based on the individual batch.\n",
    "      # tf.nn.moments is actually build on top of tf.nn.sufficient_statistics\n",
    "      # if using the conv-2d, set axis = [0,1,2] to return depth-wise normalize\n",
    "      \n",
    "      mean, variance = nn.moments(inputs, axis, shift=moving_mean)\n",
    "      \n",
    "      # =======================================================================\n",
    "      # Update the moving_mean and moving_variance moments.\n",
    "      \n",
    "      update_moving_mean = moving_averages.assign_moving_average(\n",
    "          moving_mean, mean, decay)\n",
    "      update_moving_variance = moving_averages.assign_moving_average(\n",
    "          moving_variance, variance, decay)\n",
    "      if updates_collections is None:\n",
    "        # =======================================================================\n",
    "        # Make sure the updates are computed here. ( first time )\n",
    "        # ======================================================================= \n",
    "        # control_dependencies \n",
    "        # just make sure compute update first and then compute batch_normalization\n",
    "        # \n",
    "        with ops.control_dependencies([update_moving_mean,\n",
    "                                       update_moving_variance]):\n",
    "          outputs = nn.batch_normalization(\n",
    "              inputs, mean, variance, beta, gamma, epsilon)\n",
    "      else:\n",
    "        # Collect the updates to be computed later.\n",
    "        ops.add_to_collections(updates_collections, update_moving_mean)\n",
    "        ops.add_to_collections(updates_collections, update_moving_variance)\n",
    "        \n",
    "        # =======================================================================\n",
    "        # After we get mean, variance, beta, gamma, esilon\n",
    "        # we could normalization it (Alg-1, normalization & last line )\n",
    "        \n",
    "        outputs = nn.batch_normalization(\n",
    "            inputs, mean, variance, beta, gamma, epsilon)\n",
    "    else:\n",
    "      outputs = nn.batch_normalization(\n",
    "          inputs, moving_mean, moving_variance, beta, gamma, epsilon)\n",
    "    outputs.set_shape(inputs.get_shape())\n",
    "    \n",
    "    # =======================================================================\n",
    "    # Additional setting \n",
    "    \n",
    "    if activation_fn:\n",
    "      outputs = activation_fn(outputs)\n",
    "    return utils.collect_named_outputs(outputs_collections, sc.name, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Batch-normalization in Keras\n",
    "\n",
    "There is another implementation from Keras, which is also nice. If you are going to use Batch-normalization in Conv2d with keras, I would recommend use this with parameters *mode* = 0, *axis* = 3 or 1, depending on your input tensor  is  for [b, h, w, c] or [b, c, h, w] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```python\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializations, regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class BatchNormalization(Layer):\n",
    "    '''Normalize the activations of the previous layer at each batch,\n",
    "    i.e. applies a transformation that maintains the mean activation\n",
    "    close to 0 and the activation standard deviation close to 1.\n",
    "    # Arguments\n",
    "        epsilon: small float > 0. Fuzz parameter.\n",
    "        mode: integer, 0, 1 or 2.\n",
    "            - 0: feature-wise normalization.\n",
    "                Each feature map in the input will\n",
    "                be normalized separately. The axis on which\n",
    "                to normalize is specified by the `axis` argument.\n",
    "                Note that if the input is a 4D image tensor\n",
    "                using Theano conventions (samples, channels, rows, cols)\n",
    "                then you should set `axis` to `1` to normalize along\n",
    "                the channels axis.\n",
    "                During training we use per-batch statistics to normalize\n",
    "                the data, and during testing we use running averages\n",
    "                computed during the training phase.\n",
    "            - 1: sample-wise normalization. This mode assumes a 2D input.\n",
    "            - 2: feature-wise normalization, like mode 0, but\n",
    "                using per-batch statistics to normalize the data during both\n",
    "                testing and training.\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "            Note that the order of this list is [gamma, beta, mean, std]\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializations](../initializations.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the gamma vector.\n",
    "        beta_regularizer: instance of [WeightRegularizer](../regularizers.md),\n",
    "            applied to the beta vector.\n",
    "    # Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    # Output shape\n",
    "        Same shape as input.\n",
    "    # References\n",
    "        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n",
    "    '''\n",
    "    def __init__(self, epsilon=1e-5, mode=0, axis=-1, momentum=0.99,\n",
    "                 weights=None, beta_init='zero', gamma_init='one',\n",
    "                 gamma_regularizer=None, beta_regularizer=None, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.beta_init = initializations.get(beta_init)\n",
    "        self.gamma_init = initializations.get(gamma_init)\n",
    "        self.epsilon = epsilon\n",
    "        self.mode = mode\n",
    "        self.axis = axis\n",
    "        self.momentum = momentum\n",
    "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
    "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
    "        self.initial_weights = weights\n",
    "        if self.mode == 0:\n",
    "            self.uses_learning_phase = True\n",
    "        super(BatchNormalization, self).__init__(**kwargs)\n",
    "    \n",
    "    # =======================================================================\n",
    "    # some init , similar to tf implementation\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "\n",
    "        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n",
    "        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.gamma_regularizer:\n",
    "            self.gamma_regularizer.set_param(self.gamma)\n",
    "            self.regularizers.append(self.gamma_regularizer)\n",
    "\n",
    "        if self.beta_regularizer:\n",
    "            self.beta_regularizer.set_param(self.beta)\n",
    "            self.regularizers.append(self.beta_regularizer)\n",
    "\n",
    "        self.running_mean = K.zeros(shape,\n",
    "                                    name='{}_running_mean'.format(self.name))\n",
    "        self.running_std = K.ones(shape,\n",
    "                                  name='{}_running_std'.format(self.name))\n",
    "        self.non_trainable_weights = [self.running_mean, self.running_std]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "        self.called_with = None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if self.mode == 0 or self.mode == 2:\n",
    "            assert self.built, 'Layer must be built before being called'\n",
    "            input_shape = self.input_spec[0].shape\n",
    "\n",
    "            reduction_axes = list(range(len(input_shape)))\n",
    "            del reduction_axes[self.axis]\n",
    "            broadcast_shape = [1] * len(input_shape)\n",
    "            broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "            if self.mode == 2:\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "            else:\n",
    "                # mode 0\n",
    "                if self.called_with not in {None, x}:\n",
    "                    raise Exception('You are attempting to share a '\n",
    "                                    'same `BatchNormalization` layer across '\n",
    "                                    'different data flows. '\n",
    "                                    'This is not possible. '\n",
    "                                    'You should use `mode=2` in '\n",
    "                                    '`BatchNormalization`, which has '\n",
    "                                    'a similar behavior but is shareable '\n",
    "                                    '(see docs for a description of '\n",
    "                                    'the behavior).')\n",
    "                self.called_with = x\n",
    "                x_normed, mean, std = K.normalize_batch_in_training(\n",
    "                    x, self.gamma, self.beta, reduction_axes,\n",
    "                    epsilon=self.epsilon)\n",
    "\n",
    "                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n",
    "                                K.moving_average_update(self.running_std, std, self.momentum)]\n",
    "\n",
    "                if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n",
    "                # =======================================================================\n",
    "                # Alg-1 : normalization \n",
    "                \n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, self.running_mean, self.running_std,\n",
    "                        self.beta, self.gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "                else:\n",
    "                    # need broadcasting\n",
    "                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n",
    "                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n",
    "                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
    "                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
    "                    x_normed_running = K.batch_normalization(\n",
    "                        x, broadcast_running_mean, broadcast_running_std,\n",
    "                        broadcast_beta, broadcast_gamma,\n",
    "                        epsilon=self.epsilon)\n",
    "\n",
    "                # pick the normalized form of x corresponding to the training phase\n",
    "                x_normed = K.in_train_phase(x_normed, x_normed_running)\n",
    "\n",
    "        elif self.mode == 1:\n",
    "            # sample-wise normalization\n",
    "            m = K.mean(x, axis=-1, keepdims=True)\n",
    "            std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n",
    "            x_normed = (x - m) / (std + self.epsilon)\n",
    "            x_normed = self.gamma * x_normed + self.beta\n",
    "        return x_normed\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'epsilon': self.epsilon,\n",
    "                  'mode': self.mode,\n",
    "                  'axis': self.axis,\n",
    "                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n",
    "                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n",
    "                  'momentum': self.momentum}\n",
    "        base_config = super(BatchNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Summary \n",
    "\n",
    "Batch-normalization is a nice trick. Expecially for a fully convnet. \n",
    "If you like my post, you can star my [github project](https://github.com/KentChun33333/kentchun33333.github.io) or consider to buy me a coffe. Thank you so much : )\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
